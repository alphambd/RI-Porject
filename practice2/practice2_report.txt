PRACTICAL SESSION 2 - RAPPORT COMPLET
==================================================

=== EXERCICE 1: PERFORMANCE ===
Collection | Taille (Ko) | Temps (s) | Documents | Vocabulaire
--------------------------------------------------------------------------------
Coll-1-10 | 55 | 0.02 | 10 | 5295
Coll-11-20 | 52 | 0.02 | 10 | 4500
Coll-21-50 | 103 | 0.02 | 30 | 8077
Coll-51-100 | 96 | 0.04 | 50 | 6720
Coll-101-200 | 357 | 0.09 | 100 | 17991
Coll-201-500 | 559 | 0.15 | 300 | 26995
Coll-501-1000 | 747 | 0.19 | 500 | 33402
Coll-1001-2000 | 1182 | 0.31 | 1000 | 44292
Coll-2001-5000 | 4201 | 1.17 | 3000 | 106850

=== EXERCICE 2: STATISTIQUES ===
Longueur moyenne documents: 496.3 termes
Longueur moyenne termes: 5.1 caractères
Taille vocabulaire: 106850 termes
Nombre total documents: 3000
Nombre total tokens: 1488801

=== EXERCICE 3: STOP WORDS ===
Vocabulaire original: 106850 termes
Avec stop words: 106249 termes
Réduction: 0.6%

=== EXERCICE 4: STEMMING ===
Vocabulaire original: 106850 termes
Avec stop words: 106249 termes
Avec stemming: 88557 termes
Réduction totale: 17.1%
Réduction stemming seul: 16.7%

=== OBSERVATIONS GLOBALES ===
1. Loi de Heaps: Croissance du vocabulaire observée
2. Stop words: Réduction significative du vocabulaire
3. Stemming: Réduction additionnelle importante
4. Performance: Temps d'indexation scalable
5. Tokenisation: Respect de la consigne 'terms without digits or special characters'
